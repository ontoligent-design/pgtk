#! /usr/bin/env python

#%% Import libraries

# Core
import glob
import os
import shutil
import sys
import re
import tarfile
import zipfile

# Data
import pandas as pd
import xml.etree.ElementTree as ET
import requests
import sqlite3

# Interface
import fire
from tqdm import tqdm

#%% Definitions

try:
    pgtk_home = os.environ['PGTK_HOME']
except KeyError:
    sys.exit('Please set PGTK_HOME.')

if not os.path.exists(pgtk_home):
    print('Creating new pgtk home at {0}'.format(pgtk_home))
    os.mkdir(pgtk_home)

rdf_dir = os.path.join(pgtk_home, 'cache', 'epub')
rdf_path = os.path.join(rdf_dir, '{0}', 'pg{0}.rdf')
db_name = os.path.join(pgtk_home, 'gutenberg.db')
pg_rdf_url = "https://www.gutenberg.org/cache/epub/feeds/rdf-files.tar.zip"
pg_index_url = "https://www.gutenberg.org/dirs/GUTINDEX.zip"
gut_url = 'https://www.gutenberg.org/ebooks/{}'
gut_txt = 'https://www.gutenberg.org/ebooks/{}.txt'
gut_utf8 = 'https://www.gutenberg.org/ebooks/{}.txt.utf-8'

ns = dict(
    base="http://www.gutenberg.org/",
    rdfs="http://www.w3.org/2000/01/rdf-schema#",
    rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#",
    marcrel="http://id.loc.gov/vocabulary/relators/",
    pgterms="http://www.gutenberg.org/2009/pgterms/",
    cc="http://web.resource.org/cc/",
    dcam="http://purl.org/dc/dcam/",
    dcterms="http://purl.org/dc/terms/")

xpaths = dict(
    subjects = ".//dcterms:subject/rdf:Description/rdf:value",
    bookshelves = ".//pgterms:bookshelf/rdf:Description/rdf:value",
    languages = ".//dcterms:language/rdf:Description/rdf:value",
    agents = ".//marcrel:ill/pgterms:agent/pgterms:name",
    rights = ".//dcterms:rights",
    title = ".//dcterms:title",
    types = ".//dcterms:type/rdf:Description/rdf:value",
    creators = ".//dcterms:creator/pgterms:agent/pgterms:name",
    formats = ".//dcterms:hasFormat/pgterms:file/dcterms:format/rdf:Description/rdf:value"
)

cat_fields = dict(
    ag = 'agents',
    bo = 'bookshelves',
    cr = 'creators',
    fo = 'formats',
    la = 'languages',
    ri = 'rights',
    su = 'subjects',
    ti = 'title',
    ty = 'types'
)

default_formats = ["text/plain; charset={}".format(cs) 
                   for cs in ['utf-8', 'ascii', 'iso-8859-1']] + ['text/plain']
default_formats_str = '^\s*' + '|'.join(default_formats) + '\s*'
default_formats_rgx =  re.compile(default_formats_str)

#%% Core Functions

def get_gids():
    gids = [os.path.basename(path) for path in glob.glob(os.path.join(rdf_dir, '*'))]
    gids = [int(gid) for gid in gids if not re.search(r'[^0-9]+', gid)]
    gids = sorted(gids)
    return gids

def get_rdf(gid):
    path = rdf_path.format(gid)
    rdf = open(path, 'r', encoding='utf8').read()
    return rdf

def get_text_content(gid, url_template=gut_utf8):
    gut_url = url_template.format(gid)
    r = requests.get(gut_url)
    # May want to test if file is valid, i.e. check  for:
    # <div class="header">
    #     <h1>Page Not Found</h1>
    # </div>
    return r.text

def get_metadata_items(el, xpath, ns=ns):
    items = [item.text for item in el.findall(xpath, namespaces=ns)]
    return items

def get_rdf_root(gid):
    rdf = get_rdf(gid)
    root = ET.fromstring(rdf)
    return root 

def get_metadata(gid):
    root = get_rdf_root(gid)
    md = {item : get_metadata_items(root, xpaths[item]) for item in xpaths}
    return md

def get_catalog(gids):
    data = []
    for gid in tqdm(gids):
        md = get_metadata(gid)
        for key in md.keys():
            if key == 'formats':
                vals = sorted(md[key])
            else:
                vals = md[key]
            for val in vals:
                val = val.upper()
                val = re.sub(r'[\n\r]+', ' ', val)
                data.append((gid, key, val))
    df = pd.DataFrame(data, columns=['gid','key','val'])
    return df

def get_catalog_wide(df):
    # print(df.head())
    df_wide = df.groupby(['gid', 'key']).val.apply(lambda x: '\n'.join(x))\
        .unstack().fillna('NONE GIVEN')
    return df_wide

def get_reduced_wide(df, cols=None):
    mycols = cols if cols is not None else ['title', 'creators', 'formats', 'languages', 'types']
    return df[mycols]

def get_catalog_text(df_wide):
    df_wide = df_wide.loc[df_wide.languages == 'en']
    df_wide = df_wide.loc[df_wide.types == 'Text']
    df_wide = df_wide.loc[df_wide.rights.str.match('Public')]
    txtidx = df_wide.formats.str.contains(default_formats_rgx).fillna(False)
    df_text = df_wide.loc[txtidx, ['title', 'creators', 'subjects']]
    df_text.subjects = df_text.subjects.fillna('NONE GIVEN')
    df_text.creators = df_text.creators.fillna('NONE GIVEN')
    return df_text
    
def save_catalog_to_db(catalog, tname='catalog'):
    with sqlite3.connect(db_name) as db:
        catalog.to_sql(tname, db, index=True, if_exists='replace')

def get_catalog_from_db():
    with sqlite3.connect(db_name) as db:
        catalog = pd.read_sql('select * from catalog', db, index_col='gid')
        return catalog
    
def get_gids_for_pat_from_db(catalog, name_pat, key='creators'):
    sql  = "SELECT gid FROM catalog WHERE creators LIKE ?"
    with sqlite3.connect(db_name) as db:
        df = pd.read_sql(sql, db, params=(name_pat,))
        return df.gid.tolist()

#%% Cross platform utilities

def wget(url, workingdir=None):
    stream = requests.get(url, stream=True)

    total_size = int(stream.headers.get('content-length', 0))
    progress_bar = tqdm(total=total_size, unit='iB', unit_scale=True, desc='Downloading   ')

    with open(os.path.join(workingdir, url.split('/')[-1]), 'wb') as f:
        for chunk in stream.iter_content(chunk_size=1024):
            progress_bar.update(len(chunk))
            f.write(chunk)
    progress_bar.close()


def unzip(zip, workingdir=None):
    with zipfile.ZipFile(os.path.join(workingdir, zip)) as z:
        for element in tqdm(z.infolist(), desc='Decompressing '):
            z.extract(element, workingdir)

def tar_xf(mytarfile, workingdir=None):
    with tarfile.open(os.path.join(workingdir, mytarfile)) as t:
        for element in tqdm(t.getmembers(), desc='Extracting    '):
            t.extract(element, workingdir)

def rm(file, workingdir=None, recursive=False):
    effective_file = os.path.join(workingdir, file) if workingdir else file
    if os.path.exists(effective_file):
        if recursive:
            shutil.rmtree(effective_file)
        else:
            os.remove(effective_file)


#%% User Commands

def download_cache(overwrite=True):
    """Download RDF catalog from Project Gutenburg"""
    if overwrite == True or not os.path.exists(rdf_dir):
        print("Downloading RDF cache")
        rdf_zip = pg_rdf_url.split('/')[-1]
        rdf_tar =  rdf_zip.replace('.zip', '')
        wget(pg_rdf_url, workingdir=pgtk_home)
        unzip(rdf_zip, workingdir=pgtk_home)
        tar_xf(rdf_tar, workingdir=pgtk_home)
        rm(rdf_zip, workingdir=pgtk_home)
        rm(rdf_tar, workingdir=pgtk_home)
    else:
        print("RDF files exist on system. Set '--overwrite True'.")

def populate_database(replace=False):
    """Create db with downloaded RDF data"""
    print("Creating database from RDF cache")
    if replace or not os.path.exists(db_name):
        if  os.path.exists(rdf_dir):
            print('Extracting data from catalog')
            gids = get_gids()
            cat = get_catalog(gids)
            print("Converting data into wide form")
            cat_wide = get_catalog_wide(cat)
            print("Saving data to database")
            save_catalog_to_db(cat_wide)
        else:
            print("No RDF cache. Run download-cache first.")
    else:
        print("Database exists. To overwrite set '--replace True'")

def remove_cache():
    """Remove RDF cache from file system"""
    print("Removing RDF cache in", rdf_dir)
    rm(rdf_dir, recursive=True)

def print_results(results):
    """Print search results in standard format"""
    data = []
    for idx in results.index:
        item = results.loc[idx]
        row = (str(item.gid), 
            re.sub(r'\n', ' ', item.creators),
            re.sub(r'[\n\r]+', ' ', item.title), 
            gut_utf8.format(item.gid))
        data.append(row)
    _ = [print('|'.join(row)) for row in data]
    #print('#', len(data), "items returned")
    return data
        
def run_query(where_clause, limit=1000):
    """Run a query to the catalog table in the database. 
    Don't use a LIMIT clause; instead pass argument."""
    sql = "SELECT * FROM catalog WHERE {} ORDER BY creators, title LIMIT {}"\
        .format(where_clause, limit)
    with sqlite3.connect(db_name) as db:
        results = pd.read_sql_query(sql, db)
        print_results(results)

def search_fields(ti='', cr='', su='', la='EN', ty='TEXT', fo='%TEXT/PLAIN%'):
    """Search catalog by fields:  title (ti), creators (cr), and subjects (su).
    Also: languages (la) defaults to 'EN', types (ty) defaults to 'TEXT', and 
    formats (fo) defaults to '%TEXT/PLAIN%'.
    """
    args = dict(ti=ti, cr=cr, su=su, la=la, fo=fo) #locals()
    where_clause_list = []
    params = []
    for f in args:
        if args[f] != '':
            where_clause_list.append("{} LIKE ? ".format(cat_fields[f]))            
            if isinstance(args[f], tuple):
                args[f] = ', '.join(args[f])
            if f == 'ti' or f == 'su' or f == 'cr':
                args[f] = "%{}%".format(args[f].upper())
            params.append(args[f])
    where_clause = ' AND '.join(where_clause_list)
    types_returned = '{} in ("{}", "NONE GIVEN")'.format(cat_fields['ty'], ty)
    sql = "SELECT * FROM catalog WHERE {} AND {} ORDER BY creators, title".format(where_clause, types_returned)
    with sqlite3.connect(db_name) as db:
        results = pd.read_sql_query(sql, db, params=params)
        print_results(results)

def download_epubs(epub_file, outdir=None, sep='|'):
    """Download epubs from a list of files generated from a search"""
    
    if not outdir:
        outdir = epub_file.split('.')[0]
    if not os.path.exists(outdir):
        os.mkdir(outdir)        
    
    print("Downloading files to", outdir)
    with open(epub_file, 'r') as fyle:
        for line in fyle.readlines():
            row = line.split(sep)
            gid = row[0]
            try:
                url = gut_utf8.format(gid)
                stream = requests.get(url, stream=True)
                total_size = int(stream.headers.get('content-length', 0))
                progress_bar = tqdm(total=total_size, unit='iB', unit_scale=True, desc='Downloading {}'.format(gid))
                filename = '_'.join(row[1:3]).strip()
                filename = re.sub(r'\W+', '_', filename)
                filename = re.sub(r'_+', '_', filename)

                with open(os.path.join(outdir, '{}-pg{}.txt'.format(filename, gid)), 'wb') as f:
                    for chunk in stream.iter_content(chunk_size=1024):
                        progress_bar.update(len(chunk))
                        f.write(chunk)
                progress_bar.close()
            except ValueError as e:
                print('#', gid, "not a GID")

if __name__ == '__main__':
    
    commands = {
        'getcat': download_cache,
        'mkdb': populate_database,
        'rmcat': remove_cache,
        'query': run_query,
        'search': search_fields,
        'getpubs': download_epubs,
    }
    fire.Fire(commands)
    
